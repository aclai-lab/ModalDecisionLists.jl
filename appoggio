diff --git a/src/ModalDecisionLists.jl b/src/ModalDecisionLists.jl
index b98e9fc..8a2a4de 100644
--- a/src/ModalDecisionLists.jl
+++ b/src/ModalDecisionLists.jl
@@ -26,11 +26,12 @@ include("algorithms/sequentialcovering.jl")
 
 
 module BaseCN2
-using ModalDecisionLists: RuleAntecedent, SatMask
+using ModalDecisionLists: SatMask
 include("algorithms/base-cn2.jl")
 end
 
 export SequentialCoveringLearner
+export OrderedCN2Learner
 export build_cn2
 
 # Interface
diff --git a/src/algorithms/searchmethods/beamsearch.jl b/src/algorithms/searchmethods/beamsearch.jl
index 629c115..1871894 100644
--- a/src/algorithms/searchmethods/beamsearch.jl
+++ b/src/algorithms/searchmethods/beamsearch.jl
@@ -32,14 +32,14 @@ See also
 [`RandSearch`](@ref),
 [`specializeantecedents`](@ref).
 """
-@with_kw struct BeamSearch <: SearchMethod
+@with_kw mutable struct BeamSearch <: SearchMethod
     conjuncts_search_method::SearchMethod=AtomSearch()
     beam_width::Integer=3
     loss_function::Function=entropy
     discretizedomain::Bool=false
     alphabet::Union{Nothing,AbstractAlphabet}=nothing
-    max_purity_const::Union{Real,Nothing}=nothing
-    significance_alpha::Union{Real,Nothing}=nothing
+    max_purity_const::Union{Real,Nothing}=0.0
+    significance_alpha::Union{Real,Nothing}=0.0
 end
 
 
@@ -47,7 +47,7 @@ end
     function filteralphabetoptimized(
         X::AbstractLogiset,
         alph::UnionAlphabet,
-        antecedent_info::Tuple{RuleAntecedent,SatMask}
+        antecedent_info::Tuple{Formula,SatMask}
     )::Vector{Tuple{Atom,SatMask}}
 
 Like filteralphabet but with an additional filtering step ensuring that each atom is not a
@@ -89,7 +89,7 @@ end
 """
     newconditions(
         X::AbstractLogiset,
-        antecedent::Tuple{RuleAntecedent, SatMask}
+        antecedent::Tuple{Formula, SatMask}
     )::Vector{Tuple{Atom, SatMask}}
 
 Returns the list of all possible conditions (atoms) that can be derived from instances
@@ -270,6 +270,7 @@ function findbestantecedent(
             best_lossfnctn = bestcandidate_lossfnctn
         end
     end
+
     return best
 end
 
@@ -278,56 +279,40 @@ end
 ############################################################################################
 
 function find_singlerule(
+    candidates::AbstractVector{<:Tuple{Formula, BitVector}},
     X::AbstractLogiset,
     y::AbstractVector{<:Integer},
     w::AbstractVector,
     beam_width::Integer,
     # laplace
-    target_class::Integer,
-    n_labels::Integer,
-
+    target_class,
+    n_labels,
+    # optional positional
     discretizedomain::Bool=false,
+    truerfirst::Bool=false,
     max_rule_length::Union{Nothing,Integer}=nothing,
     alphabet::Union{Nothing,AbstractAlphabet}=nothing,
     max_purity_const::Union{Nothing,Real}=nothing
-)::Tuple{Union{Truth,Formula},SatMask}
-
-    best = (⊤, ones(Bool, nrow(X)))
-    best_quality = laplace_accuracy(y, w; n_labels, target_class)
-
-    newcandidates = Tuple{Formula,SatMask}[]
+)::Tuple{Union{Truth,LeftmostConjunctiveForm},SatMask}
+
+    newcandidates = specializeantecedents(candidates,
+                        X, y,
+                        max_rule_length, truerfirst, discretizedomain, alphabet
+    )
+
+    (perm, bestcandidate_quality) = sortantecedents(newcandidates,
+                        y, w,
+                        beam_width, laplace_accuracy, max_purity_const;
+                        #
+                        target_class=target_class,
+                        n_labels=n_labels
+    )
+end
 
-    while true
-        (candidates, newcandidates) = newcandidates, Tuple{Formula,SatMask}[]
-        newcandidates = specializeantecedents(candidates,
-                            X, y,
-                            max_rule_length, discretizedomain, alphabet
-                        )
-        # In case of DecisionSet learning all the antecedents that do not cover any instances
-        # labeled with the target_class are removed.
-        newcandidates = [sant for sant in newcandidates if (
-                            (_, satmask) = sant;
-                            any(y[satmask] .== target_class)
-                        )]
-        (perm, bestcandidate_quality) = sortantecedents(newcandidates,
-                            y, w,
-                            beam_width, laplace_accuracy, max_purity_const;
-                            target_class=target_class,
-                            n_labels=n_labels
-                        )
-
-        isempty(perm) && break
-
-        newcandidates = newcandidates[perm]
-
-        if bestcandidate_quality < best_quality
-            best = newcandidates[1]
-            best_quality = bestcandidate_quality
-        end
+############################################################################################
+############################################################################################
+############################################################################################
 
-    end # end while
-    return best
-end
 
 function find_rules(
     bs::BeamSearch,
@@ -349,7 +334,7 @@ function find_rules(
     wuncovered = w
 
     initial_classdistribution = counts(y, n_labels)
-    newcandidates = Tuple{RuleAntecedent,SatMask}[]
+    newcandidates = Tuple{Formula,SatMask}[]
 
     bestrules = []
     while true
diff --git a/src/algorithms/searchmethods/randsearch.jl b/src/algorithms/searchmethods/randsearch.jl
index 4a146b4..ca80306 100644
--- a/src/algorithms/searchmethods/randsearch.jl
+++ b/src/algorithms/searchmethods/randsearch.jl
@@ -1,5 +1,5 @@
 using Parameters
-using SoleLogics
+using SoleLogics: randatom
 using FillArrays
 using Random
 using ModalDecisionLists.Measures: entropy, significance_test
@@ -11,33 +11,41 @@ using ModalDecisionLists.Measures: entropy, significance_test
 Generate random formulas (`SoleLogics.randformula`)
 ....
 """
-@with_kw struct RandSearch <: SearchMethod
+@with_kw mutable struct RandSearch <: SearchMethod
     cardinality::Integer=10
     loss_function::Function=ModalDecisionLists.Measures.entropy
     operators::AbstractVector=[NEGATION, CONJUNCTION, DISJUNCTION]
     syntaxheight::Integer=2
     discretizedomain::Bool=false
     rng::Union{Integer,AbstractRNG} = Random.GLOBAL_RNG
-    alpha::Real=1.0
+    alpha::Real=1.0 # Unused
     max_purity_const::Union{Real,Nothing}=nothing
     default_alphabet::Union{Nothing,AbstractAlphabet}=nothing
+    # randatom parameters
+    atompicking_mode::Symbol=:uniform
+    subalphabets_weights::Union{AbstractWeights,AbstractVector{<:Real},Nothing} = nothing
 end
 
 
-
-#
-# Potrei mettere in OR regole successive nella DL aventi stessa classe?
 function unaryconditions(
     rs::RandSearch,
     a::AbstractAlphabet,
     X::AbstractLogiset
 )::Vector{Tuple{Formula,SatMask}}
 
-    @unpack cardinality, operators, syntaxheight, rng = rs
+    @unpack cardinality, operators, syntaxheight, rng,
+        atompicking_mode, subalphabets_weights = rs
 
-    # TODO devo generare 10 formule comprese quelle non buone ?
+    if rng isa Integer
+        rng = MersenneTwister(rng)
+    end
     conditions = [ begin
-        formula = randformula(rng, syntaxheight, a, operators)
+        formula = randformula(rng, syntaxheight, a, operators;
+                    atompicker = ( (rng, a) -> randatom(rng, a;
+                                        atompicking_mode = atompicking_mode,
+                                        subalphabets_weights = subalphabets_weights
+                        ))
+            )
         satmask = check(formula, X)
         if any(satmask)
             (formula, satmask)
@@ -90,34 +98,47 @@ function extract_optimalantecedent(
     max_purity,
     y::AbstractVector{<:CLabel},
     w::AbstractVector;
+    min_rule_coverage::Integer=1,
     kwargs...
 )::Tuple{Formula,SatMask}
 
     # TODO mappare laplace accuracy in intervallo 0,1
     bestant_satmask = ones(Bool, length(y))
     bestformula = begin
+
         if !isempty(formulas)
-            (bestant_formula, bestant_satmask) = argmin(((rfa, satmask),) -> begin
-                relative_lossfnctn = loss_function(y[satmask], w[satmask]; kwargs...) - max_purity
-                    if relative_lossfnctn >= 0
-                        relative_lossfnctn
+            losses = map(((rfa, satmask),) -> begin
+                relative_loss = loss_function(y[satmask], w[satmask]; kwargs...) - max_purity
+                    # TODO @edo review check on min_rule_coverage and min_purity
+                    if (relative_loss >= 0) & (count(satmask) > min_rule_coverage)
+                        relative_loss
                     else Inf
                     end
             end, formulas)
+            bestindex = argmin(losses)
+
+            (bestant_formula, bestant_satmask) = formulas[bestindex]
         end
-        # Minore non minore o uguale
-        if all(bestant_satmask) | ((loss_function(y[bestant_satmask], w[bestant_satmask]; kwargs...) - max_purity) < 0)
+        if all(bestant_satmask) | (losses[bestindex] > loss_function(y, w; kwargs...))
             bestant_formula = TOP
             bestant_satmask = ones(length(y))
         end
         (bestant_formula, bestant_satmask)
-    end
+    end # bestantformula
     return bestformula
 end
 
 
 
 
+# TODO add rng parameter.
+
+function searchantecedents(
+    sm::RandSearch,
+    X::PropositionalLogiset,
+)::Tuple{Formula,SatMask}
+    return (randformula(sm.syntaxheight, alphabet(X), sm.operators) for _ in 1:sm.cardinality)
+end
 function findbestantecedent(
     rs::RandSearch,
     X::PropositionalLogiset,
@@ -157,7 +178,7 @@ function findbestantecedent(
             randformulas = unaryconditions(rs, selectedalphabet, X)
             bestantecedent = extract_optimalantecedent(randformulas,
                             loss_function, max_purity, y, w;
-                            kwargs...)
+                            min_rule_coverage, kwargs...)
         else
             (TOP, ones(Bool, length(y)))
         end
diff --git a/src/algorithms/sequentialcovering.jl b/src/algorithms/sequentialcovering.jl
index 334bbeb..99f10d6 100644
--- a/src/algorithms/sequentialcovering.jl
+++ b/src/algorithms/sequentialcovering.jl
@@ -175,7 +175,6 @@ function sequentialcovering(
     kwargs...
 )::DecisionList where {U<:Real}
 
-
     !isnothing(max_rulebase_length) && @assert max_rulebase_length > 0 "`max_rulebase_length` must be  > 0"
 
     @assert w isa AbstractVector || w in [nothing, :rebalance, :default]
diff --git a/src/core.jl b/src/core.jl
index 8057184..a371498 100644
--- a/src/core.jl
+++ b/src/core.jl
@@ -8,7 +8,6 @@ using ModalDecisionLists: Measures
 using ModalDecisionLists.Measures: laplace_accuracy
 using ModalDecisionLists.Measures: significance_test
 
-const RuleAntecedent = SoleLogics.LeftmostConjunctiveForm{SoleLogics.Atom{ScalarCondition}}
 const SatMask = BitVector
 
 
@@ -98,21 +97,25 @@ function maptointeger(y::AbstractVector{<:CLabel})
 end
 
 """
-    sortantecedents(
-        antecedents::Vector{Tuple{RuleAntecedent, SatMask}},
+    best_satmasks(
+        satmasks::Vector{Tuple{Formula, SatMask}},
         y::AbstractVector{CLabel},
         w::AbstractVector,
         beam_width::Integer,
         loss_function::Function
     )
 
+Sort rule satmasks based on their quality, using a specified loss function.
 
-Sorts rule antecedents based on their lossfnctn using a specified evaluation function.
+Sorts rule antecedents based on their lossfnctn using a specified loss function.
 
 Takes an *antecedents*, each decorated by a SatMask indicating his coverage bitmask.
-Each antecedent is evaluated on his covered y using the provided *lossfnctn evaluator* function.
+Each antecedent is evaluated on his covered y using the provided *loss_function* function.
 Then the permutation of the bests *beam_search* sorted antecedent is returned with the lossfnctn
 value of the best one.
+
+See also
+[`entropy`](@ref).
 """
 function sortantecedents(
     antecedents::AbstractVector{<:Tuple{Formula, BitVector}},
@@ -169,26 +172,3 @@ end
 ############################################################################################
 ############ Utils #########################################################################
 ############################################################################################
-
-
-"""
-Dumb utility function to preprocess input data:
-    * remove duplicated rows
-    * remove rows with missing values
-"""
-function preprocess_inputdata(
-    X::AbstractDataFrame,
-    y;
-    remove_duplicate_rows = false
-)
-    if remove_duplicate_rows
-        allunique(X) && return (X, y)
-        nonunique_ind = nonunique(X)
-        Xy = hcat( X[findall((!).(nonunique_ind)), :],
-                   y[findall((!).(nonunique_ind))]
-        ) |> dropmissing
-    else
-        Xy = hcat(X[:, :], y[:]) |> dropmissing
-    end
-    return Xy[:, 1:(end-1)], Xy[:, end]
-end
diff --git a/src/interfaces/MLJ.jl b/src/interfaces/MLJ.jl
index ce85327..256528c 100644
--- a/src/interfaces/MLJ.jl
+++ b/src/interfaces/MLJ.jl
@@ -1,12 +1,14 @@
 module MLJInterface
 
 export SequentialCoveringLearner
+export OrderedCN2Learner
 # export BeamSearch, RandSearch
 
 # using ModalDecisionTrees.MLJInterface: wrapdataset
 using ModalDecisionLists
+using ModalDecisionLists.Measures: laplace_accuracy
 import ModalDecisionLists: SearchMethod, BeamSearch, RandSearch
-import ModalDecisionLists:  sequentialcovering
+import ModalDecisionLists: sequentialcovering
 
 import SoleData: PropositionalLogiset
 import SoleBase: CLabel
@@ -30,6 +32,7 @@ abstract type CoveringStrategy <: MLJModelInterface.Deterministic end
 mutable struct SequentialCoveringLearner <: CoveringStrategy
     searchmethod::SearchMethod
     max_rulebase_length::Union{Nothing,Integer}
+    min_rule_coverage::Union{Nothing,Integer}
     suppress_parity_warning::Bool
 end
 
@@ -47,6 +50,7 @@ end
 function SequentialCoveringLearner(;
     searchmethod::SearchMethod=BeamSearch(),
     max_rulebase_length::Union{Nothing,Integer}=nothing,
+    min_rule_coverage::Integer = 1,
     suppress_parity_warning::Bool=false,
     kwargs...
 )
@@ -54,6 +58,7 @@ function SequentialCoveringLearner(;
     model =  SequentialCoveringLearner(
         searchmethod,
         max_rulebase_length,
+        min_rule_coverage,
         suppress_parity_warning
     )
     message = MMI.clean!(model)
@@ -62,8 +67,63 @@ function SequentialCoveringLearner(;
     return model
 end
 
-################ Fit #######################################################################
+function MMI.predict(m::CoveringStrategy, fitresult, Xnew)
+    yhat = apply(fitresult.model, PropositionalLogiset(Xnew))
+    return yhat
+end
+
+############################################################################################
+############################ OrderedCN2Learner #############################################
+############################################################################################
+
+mutable struct OrderedCN2Learner <: CoveringStrategy
+
+    beam_width::Integer
+    loss_function::Function
+    discretizedomain::Bool
+    max_purity_const::Union{Real,Nothing}
+    significance_alpha::Union{Real,Nothing}
+    # SequentialCovering
+    min_rule_coverage::Integer
+    max_rule_length::Union{Real,Nothing}
+    max_rulebase_length::Union{Nothing,Integer}
+end
+
+function MMI.clean!(model::OrderedCN2Learner)
+    warning = ""
+    if !isnothing(model.max_rulebase_length) && model.max_rulebase_length < 1
+        warning *= "Need max_rulebase_length ≥ 1. Resetting max_rulebase_length = nothing. "
+        #
+        model.max_rulebase_length = nothing
+    end
+    return warning
+end
 
+# Keyword constructor
+function OrderedCN2Learner(;
+    beam_width::Integer = 3,
+    loss_function::Function = ModalDecisionLists.Measures.entropy,
+    discretizedomain::Bool = false,
+    max_purity_const::Union{Real,Nothing} = nothing,
+    significance_alpha::Union{Real,Nothing} = nothing,
+    # SequentialCovering
+    min_rule_coverage::Integer = 1,
+    max_rule_length::Union{Nothing,Integer} = nothing,
+    max_rulebase_length::Union{Nothing,Integer} = nothing,
+)
+    model = OrderedCN2Learner(beam_width,
+        loss_function, discretizedomain,
+        max_purity_const, significance_alpha,
+        min_rule_coverage, max_rule_length, max_rulebase_length,
+    )
+    message = MMI.clean!(model)
+    isempty(message) || @warn message
+
+    return model
+end
+
+################ Fit (General for all CoveringStrategy ) ###################################
+############################################################################################
 function MMI.fit(m::CoveringStrategy, verbosity::Integer, X, y)
 
     # TODO wrapdataset
@@ -72,45 +132,45 @@ function MMI.fit(m::CoveringStrategy, verbosity::Integer, X, y)
 
     model = begin
         if m isa SequentialCoveringLearner
-            sequentialcovering(
-                        X_pl, y_cl;
-                        m.searchmethod,
-                        m.max_rulebase_length,
-                        m.suppress_parity_warning)
+            sequentialcovering(X_pl, y_cl;
+                        searchmethod = m.searchmethod,
+                        max_rulebase_length = m.max_rulebase_length,
+                        min_rule_coverage = m.min_rule_coverage,
+                        suppress_parity_warning = m.suppress_parity_warning
+        )
+        elseif m isa OrderedCN2Learner
+            searchmethod = BeamSearch( conjuncts_search_method = AtomSearch(),
+                beam_width          = m.beam_width,
+                loss_function       = m.loss_function,
+                discretizedomain    = m.discretizedomain,
+                max_purity_const    = m.max_purity_const,
+                significance_alpha  = m.significance_alpha,
+            )
+            sequentialcovering(X_pl, y_cl;
+                        searchmethod,
+                        m.min_rule_coverage,
+                        m.max_rule_length,
+                        m.max_rulebase_length
+            )
         else
             error("unexpected model type $(typeof(model))")
         end
     end
-
     if verbosity == 1
         println(model)
     end
-
-    # Outs ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     fitresult = (
         model = model,
     )
-    report = (
-        rulebase_length = length(rulebase(model)),
-        avg_ruleslength = mean([natoms(antecedent(rule)) for rule in rulebase(model)])
-    )
+    report = nothing
     cache = nothing
-    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
     return fitresult, cache, report
 end
 
-################ Predict ###################################################################
-
-function MMI.predict(m::SequentialCoveringLearner, fitresult, Xnew)
-    yhat = apply(fitresult.model, PropositionalLogiset(Xnew))
-    return yhat
-end
-
-############### Metadata ###################################################################
-
 MMI.metadata_pkg.(
     (
+        OrderedCN2Learner,
         SequentialCoveringLearner,
     ),
     name = "$(MDL)",
diff --git a/src/measures.jl b/src/measures.jl
index cbdc8d2..b56b1ce 100644
--- a/src/measures.jl
+++ b/src/measures.jl
@@ -14,8 +14,9 @@ using Distributions
 function laplace_accuracy(
     y::AbstractVector{<:Integer},
     w::AbstractVector=default_weights(length(y));
+    n_labels::Integer,
     target_class::Union{Integer,Nothing} = nothing,
-    n_labels::Integer
+    kwargs...
 )
     dist = counts(y, n_labels)
 
@@ -26,7 +27,7 @@ function laplace_accuracy(
             (length(dist), maximum(dist))
         end
     end
-    return -((target + 1) / (sum(dist) + k))
+    return 1 - ((target + 1) / (sum(dist) + k))
 end
 
 function entropy(
